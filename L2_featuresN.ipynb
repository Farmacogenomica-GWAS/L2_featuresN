{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Analysis of the Results File: L2_featuresN Obtained via Tierpsy Tracker"
      ],
      "metadata": {
        "id": "MhUtcxbP9dob"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Drive connection\n"
      ],
      "metadata": {
        "id": "kt-CVe2T9fs9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKfe-TOb9h-9",
        "outputId": "ab12fda6-ff15-4ed1-9e23-2b20f5541fa9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Libraries"
      ],
      "metadata": {
        "id": "PCexMhZv9siO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json"
      ],
      "metadata": {
        "id": "owS8WCIi9tyu"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inspecting file content"
      ],
      "metadata": {
        "id": "OxFvO3199uQ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def inspect_hdf5_datasets(file_path):\n",
        "    \"\"\"\n",
        "    Inspects an HDF5 file and prints the names and sizes of its datasets.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): The path to the HDF5 file.\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*30}\")\n",
        "    print(f\"Inspecting file: {file_path}\")\n",
        "    print(f\"{'='*30}\")\n",
        "    try:\n",
        "        if not os.path.exists(file_path):\n",
        "            print(f\"Error: The file '{file_path}' was not found.\")\n",
        "            return  # Exit the function if the file doesn't exist\n",
        "\n",
        "        with h5py.File(file_path, 'r') as hdfid:\n",
        "            datasets_info = {}\n",
        "            for name, obj in hdfid.items():\n",
        "                if isinstance(obj, h5py.Dataset):\n",
        "                    datasets_info[name] = obj.size\n",
        "\n",
        "            if datasets_info:\n",
        "                print(\"Datasets found and their sizes:\")\n",
        "                for name, size in datasets_info.items():\n",
        "                    print(f\"  Dataset: {name}, Size: {size} elements\")\n",
        "            else:\n",
        "                print(\"No top-level datasets were found in this file.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while processing the file '{file_path}': {e}\")\n",
        "\n",
        "# Define the file path you want to analyze here.  REPLACE THIS!\n",
        "file_to_analyze = '/content/drive/MyDrive/Worms/Resultados/L2/L2_featuresN/L2_featuresN.hdf5'  # <--- REPLACE THIS LINE\n",
        "\n",
        "inspect_hdf5_datasets(file_to_analyze)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VISTVdqC9vyN",
        "outputId": "827ded93-7c1f-4162-d90f-7ae5acfcee50"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==============================\n",
            "Inspecting file: /content/drive/MyDrive/Worms/Resultados/L2/L2_featuresN/L2_featuresN.hdf5\n",
            "==============================\n",
            "Datasets found and their sizes:\n",
            "  Dataset: blob_features, Size: 14636 elements\n",
            "  Dataset: features_stats, Size: 4539 elements\n",
            "  Dataset: timeseries_data, Size: 14636 elements\n",
            "  Dataset: trajectories_data, Size: 14636 elements\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exporting the datasets to individuals CSV files"
      ],
      "metadata": {
        "id": "_rHvRbwb936-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def export_hdf5_datasets_to_csv(file_path, output_dir):\n",
        "    \"\"\"\n",
        "    Exports all datasets from an HDF5 file to individual CSV files.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): The path to the HDF5 file.\n",
        "        output_dir (str): The path to the directory where the CSV files will be saved.\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*30}\")\n",
        "    print(f\"Exporting all datasets from: {file_path}\")\n",
        "    print(f\"CSV files will be saved in: {output_dir}\")\n",
        "    print(f\"{'='*30}\")\n",
        "\n",
        "    try:\n",
        "        if not os.path.exists(file_path):\n",
        "            print(f\"Error: The file '{file_path}' was not found.\")\n",
        "            return\n",
        "\n",
        "        # Create the output directory if it doesn't exist\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        with h5py.File(file_path, 'r') as hdf_file:\n",
        "            for dataset_name in hdf_file:\n",
        "                if isinstance(hdf_file[dataset_name], h5py.Dataset):\n",
        "                    print(f\"\\nProcessing dataset: {dataset_name}\")\n",
        "                    dataset = hdf_file[dataset_name]\n",
        "                    data = dataset[:]  # Read all data\n",
        "\n",
        "                    # Convert to Pandas DataFrame\n",
        "                    df = pd.DataFrame(data)\n",
        "\n",
        "                    # Construct the CSV file path\n",
        "                    csv_file_path = os.path.join(output_dir, f\"{dataset_name}.csv\")\n",
        "\n",
        "                    # Export to CSV\n",
        "                    df.to_csv(csv_file_path, index=False)\n",
        "                    print(f\"Dataset '{dataset_name}' successfully exported to: {csv_file_path}\")\n",
        "                else:\n",
        "                    print(f\"Skipping: '{dataset_name}' is not a dataset.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "    print(\"\\nData export process complete.\")\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Example usage:  MODIFY THESE PATHS APPROPRIATELY\n",
        "    hdf5_file_path = '/content/drive/MyDrive/Worms/Resultados/L2/L2_featuresN/L2_featuresN.hdf5'  # <--- Replace with your HDF5 file path\n",
        "    csv_output_directory = '/content/drive/MyDrive/Worms/Resultados/L2/L2_featuresN/Datasets'  # <--- Replace with the desired output folder\n",
        "\n",
        "    export_hdf5_datasets_to_csv(hdf5_file_path, csv_output_directory)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IWPSnbU3-yqs",
        "outputId": "78650e90-6faa-4c62-d9bc-68dfded6ae71"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==============================\n",
            "Exporting all datasets from: /content/drive/MyDrive/Worms/Resultados/L2/L2_featuresN/L2_featuresN.hdf5\n",
            "CSV files will be saved in: /content/drive/MyDrive/Worms/Resultados/L2/L2_featuresN/Datasets\n",
            "==============================\n",
            "\n",
            "Processing dataset: blob_features\n",
            "Dataset 'blob_features' successfully exported to: /content/drive/MyDrive/Worms/Resultados/L2/L2_featuresN/Datasets/blob_features.csv\n",
            "Skipping: 'coordinates' is not a dataset.\n",
            "\n",
            "Processing dataset: features_stats\n",
            "Dataset 'features_stats' successfully exported to: /content/drive/MyDrive/Worms/Resultados/L2/L2_featuresN/Datasets/features_stats.csv\n",
            "Skipping: 'provenance_tracking' is not a dataset.\n",
            "\n",
            "Processing dataset: timeseries_data\n",
            "Dataset 'timeseries_data' successfully exported to: /content/drive/MyDrive/Worms/Resultados/L2/L2_featuresN/Datasets/timeseries_data.csv\n",
            "\n",
            "Processing dataset: trajectories_data\n",
            "Dataset 'trajectories_data' successfully exported to: /content/drive/MyDrive/Worms/Resultados/L2/L2_featuresN/Datasets/trajectories_data.csv\n",
            "\n",
            "Data export process complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A problem arose because 'coordinates' and 'provenance_tracking' were not datasets. They were extracted separately:"
      ],
      "metadata": {
        "id": "IaHo_1Jh_i9o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Note on non-dataset exports:\n",
        "#\n",
        "# The script exports standard HDF5 datasets to CSV files.  However, the following non-dataset items were handled specially:\n",
        "#\n",
        "# -  'coordinates': This HDF5 group contains 3D datasets ('dorsal_contours', 'skeletons', 'ventral_contours').\n",
        "#    These datasets were extracted and stored as JSON strings within individual CSV files to preserve their 3D structure.\n",
        "#\n",
        "# -  'provenance_tracking': This HDF5 group contains metadata attributes ('CLASS', 'TITLE', 'VERSION').\n",
        "#    These attributes were extracted and written to a single-row CSV file."
      ],
      "metadata": {
        "id": "KYGmxBd3_rgI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Coordinates extraction"
      ],
      "metadata": {
        "id": "ETXDtuHtClgf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_coordinates(file_path, output_dir):\n",
        "    \"\"\"\n",
        "    Extracts the 'dorsal_contours', 'skeletons', and 'ventral_contours' datasets from an HDF5 file,\n",
        "    handling their 3D structure by storing them as JSON strings in a CSV file.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): The path to the HDF5 file.\n",
        "        output_dir (str): The path to the directory where the CSV files will be saved.\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*30}\")\n",
        "    print(f\"Extracting coordinates from: {file_path}\")\n",
        "    print(f\"CSV files will be saved in: {output_dir}\")\n",
        "    print(f\"{'='*30}\")\n",
        "\n",
        "    try:\n",
        "        if not os.path.exists(file_path):\n",
        "            print(f\"Error: The file '{file_path}' was not found.\")\n",
        "            return\n",
        "\n",
        "        # Create the output directory if it doesn't exist\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        with h5py.File(file_path, 'r') as hdf_file:\n",
        "            if 'coordinates' in hdf_file:\n",
        "                coordinates_group = hdf_file['coordinates']\n",
        "                for dataset_name in ['dorsal_contours', 'skeletons', 'ventral_contours']:\n",
        "                    if dataset_name in coordinates_group:\n",
        "                        print(f\"\\nProcessing dataset: {dataset_name}\")\n",
        "                        data = coordinates_group[dataset_name][:]  # Read all data\n",
        "\n",
        "                        if data.ndim > 2:\n",
        "                            # Store 3D coordinates as JSON strings in CSV\n",
        "                            print(f\"  Storing 3D coordinates '{dataset_name}' as JSON strings in CSV.\")\n",
        "                            # Convert each 2D slice to a JSON string\n",
        "                            json_data = [json.dumps(slice_2d.tolist()) for slice_2d in data]\n",
        "                            df = pd.DataFrame({dataset_name: json_data})  # Store in a DataFrame\n",
        "                            csv_file_path = os.path.join(output_dir, f\"{dataset_name}.csv\")\n",
        "                            df.to_csv(csv_file_path, index=False)\n",
        "                            print(f\"  Dataset '{dataset_name}' successfully exported to: {csv_file_path}\")\n",
        "                        else:\n",
        "                            print(f\"  Warning: Dataset '{dataset_name}' is not 3D. Skipping.\")\n",
        "                    else:\n",
        "                        print(f\"  Warning: Dataset '{dataset_name}' not found in 'coordinates' group.\")\n",
        "            else:\n",
        "                print(f\"  Warning: 'coordinates' group not found in HDF5 file.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "    print(\"\\nCoordinate extraction process complete.\")\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Example usage:\n",
        "    hdf5_file_path = '/content/drive/MyDrive/Worms/Resultados/L2/L2_featuresN/L2_featuresN.hdf5'  # <--- Replace with your HDF5 file path\n",
        "    csv_output_directory = '/content/drive/MyDrive/Worms/Resultados/L2/L2_featuresN/Datasets'  # <--- Replace with the desired output folder\n",
        "\n",
        "    extract_coordinates(hdf5_file_path, csv_output_directory)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJOnNoLABN4E",
        "outputId": "a27b913a-0f1f-4222-ada1-f6947f85a1ce"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==============================\n",
            "Extracting coordinates from: /content/drive/MyDrive/Worms/Resultados/L2/L2_featuresN/L2_featuresN.hdf5\n",
            "CSV files will be saved in: /content/drive/MyDrive/Worms/Resultados/L2/L2_featuresN/Datasets\n",
            "==============================\n",
            "\n",
            "Processing dataset: dorsal_contours\n",
            "  Storing 3D coordinates 'dorsal_contours' as JSON strings in CSV.\n",
            "  Dataset 'dorsal_contours' successfully exported to: /content/drive/MyDrive/Worms/Resultados/L2/L2_featuresN/Datasets/dorsal_contours.csv\n",
            "\n",
            "Processing dataset: skeletons\n",
            "  Storing 3D coordinates 'skeletons' as JSON strings in CSV.\n",
            "  Dataset 'skeletons' successfully exported to: /content/drive/MyDrive/Worms/Resultados/L2/L2_featuresN/Datasets/skeletons.csv\n",
            "\n",
            "Processing dataset: ventral_contours\n",
            "  Storing 3D coordinates 'ventral_contours' as JSON strings in CSV.\n",
            "  Dataset 'ventral_contours' successfully exported to: /content/drive/MyDrive/Worms/Resultados/L2/L2_featuresN/Datasets/ventral_contours.csv\n",
            "\n",
            "Coordinate extraction process complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Provenance_tracking extraction"
      ],
      "metadata": {
        "id": "286UHkBJCuFm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_provenance_tracking(file_path, output_dir):\n",
        "    \"\"\"\n",
        "    Extracts the attributes from the 'provenance_tracking' group in an HDF5 file\n",
        "    and saves them to a CSV file.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): The path to the HDF5 file.\n",
        "        output_dir (str): The directory where the CSV file will be saved.\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*30}\")\n",
        "    print(f\"Extracting provenance_tracking from: {file_path}\")\n",
        "    print(f\"CSV file will be saved in: {output_dir}\")\n",
        "    print(f\"{'='*30}\")\n",
        "\n",
        "    try:\n",
        "        if not os.path.exists(file_path):\n",
        "            print(f\"Error: The file '{file_path}' was not found.\")\n",
        "            return\n",
        "\n",
        "        # Create the output directory if it doesn't exist\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        with h5py.File(file_path, 'r') as hdf_file:\n",
        "            if 'provenance_tracking' in hdf_file:\n",
        "                provenance_group = hdf_file['provenance_tracking']\n",
        "                attributes = {}\n",
        "                for attr_name, attr_value in provenance_group.attrs.items():\n",
        "                    attributes[attr_name] = attr_value\n",
        "                df = pd.DataFrame([attributes])  # Create a DataFrame with a single row\n",
        "                csv_file_path = os.path.join(output_dir, \"provenance_tracking.csv\")\n",
        "                df.to_csv(csv_file_path, index=False)\n",
        "                print(f\"Provenance tracking data successfully exported to: {csv_file_path}\")\n",
        "            else:\n",
        "                print(f\"Warning: 'provenance_tracking' group not found in HDF5 file.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "    print(\"\\nProvenance tracking extraction process complete.\")\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Example usage:\n",
        "    hdf5_file_path = '/content/drive/MyDrive/Worms/Resultados/L2/L2_featuresN/L2_featuresN.hdf5'  # <--- Replace with your HDF5 file path\n",
        "    csv_output_directory = '/content/drive/MyDrive/Worms/Resultados/L2/L2_featuresN/Datasets'  # <--- Replace with the desired output folder\n",
        "\n",
        "    extract_provenance_tracking(hdf5_file_path, csv_output_directory)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJrT3cMeCx8k",
        "outputId": "c1145ee1-5de6-4e8a-f1e0-69e351329e84"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==============================\n",
            "Extracting provenance_tracking from: /content/drive/MyDrive/Worms/Resultados/L2/L2_featuresN/L2_featuresN.hdf5\n",
            "CSV file will be saved in: /content/drive/MyDrive/Worms/Resultados/L2/L2_featuresN/Datasets\n",
            "==============================\n",
            "Provenance tracking data successfully exported to: /content/drive/MyDrive/Worms/Resultados/L2/L2_featuresN/Datasets/provenance_tracking.csv\n",
            "\n",
            "Provenance tracking extraction process complete.\n"
          ]
        }
      ]
    }
  ]
}
