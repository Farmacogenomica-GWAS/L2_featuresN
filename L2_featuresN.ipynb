{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Analysis of the Results File: L2_features Obtained via Tierpsy Tracker"
      ],
      "metadata": {
        "id": "MhUtcxbP9dob"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Drive connection\n"
      ],
      "metadata": {
        "id": "kt-CVe2T9fs9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKfe-TOb9h-9",
        "outputId": "eea5d664-25d0-4554-c3ff-bbc451aea812"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Libraries"
      ],
      "metadata": {
        "id": "PCexMhZv9siO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json"
      ],
      "metadata": {
        "id": "owS8WCIi9tyu"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inspecting file content"
      ],
      "metadata": {
        "id": "OxFvO3199uQ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def inspect_hdf5_datasets(file_path):\n",
        "    \"\"\"\n",
        "    Inspects an HDF5 file and prints the names and sizes of its datasets.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): The path to the HDF5 file.\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*30}\")\n",
        "    print(f\"Inspecting file: {file_path}\")\n",
        "    print(f\"{'='*30}\")\n",
        "    try:\n",
        "        if not os.path.exists(file_path):\n",
        "            print(f\"Error: The file '{file_path}' was not found.\")\n",
        "            return  # Exit the function if the file doesn't exist\n",
        "\n",
        "        with h5py.File(file_path, 'r') as hdfid:\n",
        "            datasets_info = {}\n",
        "            for name, obj in hdfid.items():\n",
        "                if isinstance(obj, h5py.Dataset):\n",
        "                    datasets_info[name] = obj.size\n",
        "\n",
        "            if datasets_info:\n",
        "                print(\"Datasets found and their sizes:\")\n",
        "                for name, size in datasets_info.items():\n",
        "                    print(f\"  Dataset: {name}, Size: {size} elements\")\n",
        "            else:\n",
        "                print(\"No top-level datasets were found in this file.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while processing the file '{file_path}': {e}\")\n",
        "\n",
        "# Define the file path you want to analyze here.  REPLACE THIS!\n",
        "file_to_analyze = '/content/drive/MyDrive/Worms/Resultados/L2/L2_featuresN/L2_featuresN.hdf5'  # <--- REPLACE THIS LINE\n",
        "\n",
        "inspect_hdf5_datasets(file_to_analyze)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VISTVdqC9vyN",
        "outputId": "f451a12c-d03b-4dc1-919b-3180410400c8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==============================\n",
            "Inspecting file: /content/drive/MyDrive/Worms/Resultados/L2/L2_featuresN/L2_featuresN.hdf5\n",
            "==============================\n",
            "Datasets found and their sizes:\n",
            "  Dataset: blob_features, Size: 14636 elements\n",
            "  Dataset: features_stats, Size: 4539 elements\n",
            "  Dataset: timeseries_data, Size: 14636 elements\n",
            "  Dataset: trajectories_data, Size: 14636 elements\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exporting the datasets to individuals CSV files"
      ],
      "metadata": {
        "id": "_rHvRbwb936-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def export_hdf5_datasets_to_csv(file_path, output_dir):\n",
        "    \"\"\"\n",
        "    Exports all datasets from an HDF5 file to individual CSV files.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): The path to the HDF5 file.\n",
        "        output_dir (str): The path to the directory where the CSV files will be saved.\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*30}\")\n",
        "    print(f\"Exporting all datasets from: {file_path}\")\n",
        "    print(f\"CSV files will be saved in: {output_dir}\")\n",
        "    print(f\"{'='*30}\")\n",
        "\n",
        "    try:\n",
        "        if not os.path.exists(file_path):\n",
        "            print(f\"Error: The file '{file_path}' was not found.\")\n",
        "            return\n",
        "\n",
        "        # Create the output directory if it doesn't exist\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        with h5py.File(file_path, 'r') as hdf_file:\n",
        "            for dataset_name in hdf_file:\n",
        "                if isinstance(hdf_file[dataset_name], h5py.Dataset):\n",
        "                    print(f\"\\nProcessing dataset: {dataset_name}\")\n",
        "                    dataset = hdf_file[dataset_name]\n",
        "                    data = dataset[:]  # Read all data\n",
        "\n",
        "                    # Convert to Pandas DataFrame\n",
        "                    df = pd.DataFrame(data)\n",
        "\n",
        "                    # Construct the CSV file path\n",
        "                    csv_file_path = os.path.join(output_dir, f\"{dataset_name}.csv\")\n",
        "\n",
        "                    # Export to CSV\n",
        "                    df.to_csv(csv_file_path, index=False)\n",
        "                    print(f\"Dataset '{dataset_name}' successfully exported to: {csv_file_path}\")\n",
        "                else:\n",
        "                    print(f\"Skipping: '{dataset_name}' is not a dataset.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "    print(\"\\nData export process complete.\")\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Example usage:  MODIFY THESE PATHS APPROPRIATELY\n",
        "    hdf5_file_path = '/content/drive/MyDrive/Worms/Resultados/L2/L2_featuresN/L2_featuresN.hdf5'  # <--- Replace with your HDF5 file path\n",
        "    csv_output_directory = '/content/drive/MyDrive/Worms/Resultados/L2/L2_featuresN/Datasets'  # <--- Replace with the desired output folder\n",
        "\n",
        "    export_hdf5_datasets_to_csv(hdf5_file_path, csv_output_directory)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IWPSnbU3-yqs",
        "outputId": "b69eba29-492e-424a-b336-02671f4fee3b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==============================\n",
            "Exporting all datasets from: /content/drive/MyDrive/Worms/Resultados/L2/L2_featuresN/L2_featuresN.hdf5\n",
            "CSV files will be saved in: /content/drive/MyDrive/Worms/Resultados/L2/L2_featuresN/Datasets\n",
            "==============================\n",
            "\n",
            "Processing dataset: blob_features\n",
            "Dataset 'blob_features' successfully exported to: /content/drive/MyDrive/Worms/Resultados/L2/L2_featuresN/Datasets/blob_features.csv\n",
            "Skipping: 'coordinates' is not a dataset.\n",
            "\n",
            "Processing dataset: features_stats\n",
            "Dataset 'features_stats' successfully exported to: /content/drive/MyDrive/Worms/Resultados/L2/L2_featuresN/Datasets/features_stats.csv\n",
            "Skipping: 'provenance_tracking' is not a dataset.\n",
            "\n",
            "Processing dataset: timeseries_data\n",
            "Dataset 'timeseries_data' successfully exported to: /content/drive/MyDrive/Worms/Resultados/L2/L2_featuresN/Datasets/timeseries_data.csv\n",
            "\n",
            "Processing dataset: trajectories_data\n",
            "Dataset 'trajectories_data' successfully exported to: /content/drive/MyDrive/Worms/Resultados/L2/L2_featuresN/Datasets/trajectories_data.csv\n",
            "\n",
            "Data export process complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A problem arose because 'coordinates' and 'provenance_tracking' were not datasets. They were extracted separately:"
      ],
      "metadata": {
        "id": "IaHo_1Jh_i9o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Note on non-dataset exports:\n",
        "#\n",
        "# The script exports standard HDF5 datasets to CSV files.  However, the following non-dataset items were handled specially:\n",
        "#\n",
        "# -  'coordinates': This HDF5 group contains 3D datasets ('dorsal_contours', 'skeletons', 'ventral_contours').\n",
        "#    These datasets were extracted and stored as JSON strings within individual CSV files to preserve their 3D structure.\n",
        "#\n",
        "# -  'provenance_tracking': This HDF5 group contains metadata attributes ('CLASS', 'TITLE', 'VERSION').\n",
        "#    These attributes were extracted and written to a single-row CSV file."
      ],
      "metadata": {
        "id": "KYGmxBd3_rgI"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Coordinates extraction"
      ],
      "metadata": {
        "id": "ETXDtuHtClgf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_coordinates(file_path, output_dir):\n",
        "    \"\"\"\n",
        "    Extracts the 'dorsal_contours', 'skeletons', and 'ventral_contours' datasets from an HDF5 file,\n",
        "    handling their 3D structure by storing them as JSON strings in a CSV file.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): The path to the HDF5 file.\n",
        "        output_dir (str): The path to the directory where the CSV files will be saved.\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*30}\")\n",
        "    print(f\"Extracting coordinates from: {file_path}\")\n",
        "    print(f\"CSV files will be saved in: {output_dir}\")\n",
        "    print(f\"{'='*30}\")\n",
        "\n",
        "    try:\n",
        "        if not os.path.exists(file_path):\n",
        "            print(f\"Error: The file '{file_path}' was not found.\")\n",
        "            return\n",
        "\n",
        "        # Create the output directory if it doesn't exist\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        with h5py.File(file_path, 'r') as hdf_file:\n",
        "            if 'coordinates' in hdf_file:\n",
        "                coordinates_group = hdf_file['coordinates']\n",
        "                for dataset_name in ['dorsal_contours', 'skeletons', 'ventral_contours']:\n",
        "                    if dataset_name in coordinates_group:\n",
        "                        print(f\"\\nProcessing dataset: {dataset_name}\")\n",
        "                        data = coordinates_group[dataset_name][:]  # Read all data\n",
        "\n",
        "                        if data.ndim > 2:\n",
        "                            # Store 3D coordinates as JSON strings in CSV\n",
        "                            print(f\"  Storing 3D coordinates '{dataset_name}' as JSON strings in CSV.\")\n",
        "                            # Convert each 2D slice to a JSON string\n",
        "                            json_data = [json.dumps(slice_2d.tolist()) for slice_2d in data]\n",
        "                            df = pd.DataFrame({dataset_name: json_data})  # Store in a DataFrame\n",
        "                            csv_file_path = os.path.join(output_dir, f\"{dataset_name}.csv\")\n",
        "                            df.to_csv(csv_file_path, index=False)\n",
        "                            print(f\"  Dataset '{dataset_name}' successfully exported to: {csv_file_path}\")\n",
        "                        else:\n",
        "                            print(f\"  Warning: Dataset '{dataset_name}' is not 3D. Skipping.\")\n",
        "                    else:\n",
        "                        print(f\"  Warning: Dataset '{dataset_name}' not found in 'coordinates' group.\")\n",
        "            else:\n",
        "                print(f\"  Warning: 'coordinates' group not found in HDF5 file.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "    print(\"\\nCoordinate extraction process complete.\")\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Example usage:\n",
        "    hdf5_file_path = '/content/drive/MyDrive/Worms/Resultados/L2/L2_featuresN/L2_featuresN.hdf5'  # <--- Replace with your HDF5 file path\n",
        "    csv_output_directory = '/content/drive/MyDrive/Worms/Resultados/L2/L2_featuresN/Datasets'  # <--- Replace with the desired output folder\n",
        "\n",
        "    extract_coordinates(hdf5_file_path, csv_output_directory)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJOnNoLABN4E",
        "outputId": "46b5b425-3b69-4c60-c176-76c9b99a88ed"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==============================\n",
            "Extracting coordinates from: /content/drive/MyDrive/Worms/Resultados/L2/L2_featuresN/L2_featuresN.hdf5\n",
            "CSV files will be saved in: /content/drive/MyDrive/Worms/Resultados/L2/L2_featuresN/Datasets\n",
            "==============================\n",
            "\n",
            "Processing dataset: dorsal_contours\n",
            "  Storing 3D coordinates 'dorsal_contours' as JSON strings in CSV.\n",
            "  Dataset 'dorsal_contours' successfully exported to: /content/drive/MyDrive/Worms/Resultados/L2/L2_featuresN/Datasets/dorsal_contours.csv\n",
            "\n",
            "Processing dataset: skeletons\n",
            "  Storing 3D coordinates 'skeletons' as JSON strings in CSV.\n",
            "  Dataset 'skeletons' successfully exported to: /content/drive/MyDrive/Worms/Resultados/L2/L2_featuresN/Datasets/skeletons.csv\n",
            "\n",
            "Processing dataset: ventral_contours\n",
            "  Storing 3D coordinates 'ventral_contours' as JSON strings in CSV.\n",
            "  Dataset 'ventral_contours' successfully exported to: /content/drive/MyDrive/Worms/Resultados/L2/L2_featuresN/Datasets/ventral_contours.csv\n",
            "\n",
            "Coordinate extraction process complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Provenance_tracking extraction"
      ],
      "metadata": {
        "id": "286UHkBJCuFm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_provenance_tracking(file_path, output_dir):\n",
        "    \"\"\"\n",
        "    Extracts the attributes from the 'provenance_tracking' group in an HDF5 file\n",
        "    and saves them to a CSV file.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): The path to the HDF5 file.\n",
        "        output_dir (str): The directory where the CSV file will be saved.\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*30}\")\n",
        "    print(f\"Extracting provenance_tracking from: {file_path}\")\n",
        "    print(f\"CSV file will be saved in: {output_dir}\")\n",
        "    print(f\"{'='*30}\")\n",
        "\n",
        "    try:\n",
        "        if not os.path.exists(file_path):\n",
        "            print(f\"Error: The file '{file_path}' was not found.\")\n",
        "            return\n",
        "\n",
        "        # Create the output directory if it doesn't exist\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        with h5py.File(file_path, 'r') as hdf_file:\n",
        "            if 'provenance_tracking' in hdf_file:\n",
        "                provenance_group = hdf_file['provenance_tracking']\n",
        "                attributes = {}\n",
        "                for attr_name, attr_value in provenance_group.attrs.items():\n",
        "                    attributes[attr_name] = attr_value\n",
        "                df = pd.DataFrame([attributes])  # Create a DataFrame with a single row\n",
        "                csv_file_path = os.path.join(output_dir, \"provenance_tracking.csv\")\n",
        "                df.to_csv(csv_file_path, index=False)\n",
        "                print(f\"Provenance tracking data successfully exported to: {csv_file_path}\")\n",
        "            else:\n",
        "                print(f\"Warning: 'provenance_tracking' group not found in HDF5 file.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "    print(\"\\nProvenance tracking extraction process complete.\")\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Example usage:\n",
        "    hdf5_file_path = '/content/drive/MyDrive/Worms/Resultados/L2/L2_featuresN/L2_featuresN.hdf5'  # <--- Replace with your HDF5 file path\n",
        "    csv_output_directory = '/content/drive/MyDrive/Worms/Resultados/L2/L2_featuresN/Datasets'  # <--- Replace with the desired output folder\n",
        "\n",
        "    extract_provenance_tracking(hdf5_file_path, csv_output_directory)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJrT3cMeCx8k",
        "outputId": "02d1ac23-f327-42c4-bf44-07a70d6e57bb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==============================\n",
            "Extracting provenance_tracking from: /content/drive/MyDrive/Worms/Resultados/L2/L2_featuresN/L2_featuresN.hdf5\n",
            "CSV file will be saved in: /content/drive/MyDrive/Worms/Resultados/L2/L2_featuresN/Datasets\n",
            "==============================\n",
            "Provenance tracking data successfully exported to: /content/drive/MyDrive/Worms/Resultados/L2/L2_featuresN/Datasets/provenance_tracking.csv\n",
            "\n",
            "Provenance tracking extraction process complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualization of each dataset"
      ],
      "metadata": {
        "id": "-KQx9EIQiPPV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Top-Level Datasets"
      ],
      "metadata": {
        "id": "AckY40wjiR3_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "csv_output_directory = '/content/drive/MyDrive/Worms/Resultados/L2/L2_featuresN/Datasets'\n",
        "\n",
        "print(f\"\\n{'='*30}\")\n",
        "print(f\"Visualizing the first 5 rows of the datasets in: {csv_output_directory}\")\n",
        "print(f\"{'='*30}\")\n",
        "try:\n",
        "    # Check if the output directory exists\n",
        "    if not os.path.exists(csv_output_directory):\n",
        "        print(f\"Error: The directory '{csv_output_directory}' was not found.\")\n",
        "        exit()\n",
        "\n",
        "    # Iterate through all files in the specified directory\n",
        "    for filename in os.listdir(csv_output_directory):\n",
        "        if filename.endswith(\".csv\"):  # Check if the file is a CSV file\n",
        "            file_path = os.path.join(csv_output_directory, filename)\n",
        "            try:\n",
        "                # Read the CSV file into a Pandas DataFrame\n",
        "                df = pd.read_csv(file_path)\n",
        "\n",
        "                print(f\"\\n--- File: {filename} ---\")\n",
        "                print(\"First 5 rows:\")\n",
        "                if not df.empty:\n",
        "                    print(df.head())\n",
        "                else:\n",
        "                    print(\"The DataFrame is empty.\")\n",
        "\n",
        "            except pd.errors.EmptyDataError:\n",
        "                print(f\"Warning: The file '{filename}' is empty.\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error reading the file '{filename}': {e}\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "\n",
        "print(\"\\nDataset visualization process completed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_gANgzSiVB6",
        "outputId": "5cb3b0c6-70c0-4a0b-c80f-57b34f31828f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==============================\n",
            "Visualizing the first 5 rows of the datasets in: /content/drive/MyDrive/Worms/Resultados/L2/L2_featuresN/Datasets\n",
            "==============================\n",
            "\n",
            "--- File: features_stats.csv ---\n",
            "First 5 rows:\n",
            "                                                name     value\n",
            "0                                      b'speed_10th' -9.309922\n",
            "1                       b'angular_velocity_abs_10th'  0.013728\n",
            "2         b'relative_to_body_speed_midbody_abs_10th'  0.139778\n",
            "3  b'relative_to_body_radial_velocity_head_tip_10th' -6.164963\n",
            "4  b'relative_to_body_angular_velocity_head_tip_a...  0.033920\n",
            "\n",
            "--- File: trajectories_data.csv ---\n",
            "First 5 rows:\n",
            "   timestamp_raw  timestamp_time  worm_index_joined    coord_x    coord_y  \\\n",
            "0              0             0.0                  1  1206.1321  1561.0377   \n",
            "1              1             0.1                  1  1205.3759  1561.5223   \n",
            "2              2             0.2                  1  1207.4108  1561.4741   \n",
            "3              3             0.3                  1  1211.7490  1558.0664   \n",
            "4              4             0.4                  1  1213.7590  1557.5155   \n",
            "\n",
            "   threshold  roi_size   area  frame_number  was_skeletonized  skeleton_id  \\\n",
            "0      123.9      64.0  327.0             0                 1            0   \n",
            "1      123.9      64.0  333.0             1                 1            1   \n",
            "2      123.9      64.0  337.5             2                 1            2   \n",
            "3      123.9      64.0  319.0             3                 1            3   \n",
            "4      123.9      64.0  324.0             4                 1            4   \n",
            "\n",
            "   old_trajectory_data_index  worm_label  worm_index_manual  has_skeleton  \n",
            "0                          0         0.0                  1             1  \n",
            "1                          1         0.0                  1             1  \n",
            "2                          2         0.0                  1             1  \n",
            "3                          3         0.0                  1             1  \n",
            "4                          4         0.0                  1             1  \n",
            "\n",
            "--- File: blob_features.csv ---\n",
            "First 5 rows:\n",
            "       coord_x      coord_y   area   perimeter  box_length  box_width  \\\n",
            "0  1206.132080  1561.037720  330.0  129.539108   57.305859  10.295630   \n",
            "1  1205.375854  1561.522339  332.0  132.367538   58.100727  10.375130   \n",
            "2  1207.410767  1561.474121  340.5  132.610168   57.216904  12.091780   \n",
            "3  1211.749023  1558.066406  323.0  125.195961   55.068447  10.612860   \n",
            "4  1213.385986  1557.733154  328.0  128.367538   54.058155  11.301107   \n",
            "\n",
            "   quirkiness  compactness  box_orientation  solidity  intensity_mean  \\\n",
            "0    0.983729     0.247128       -29.054605  0.696202      108.012985   \n",
            "1    0.983927     0.238114       -28.610462  0.682426      107.778350   \n",
            "2    0.977414     0.243317       -28.810795  0.651675      108.335860   \n",
            "3    0.981254     0.258959       -30.068584  0.680000      108.722664   \n",
            "4    0.977904     0.250134       -30.256441  0.622982      108.795815   \n",
            "\n",
            "   intensity_std       hu0       hu1       hu2       hu3           hu4  \\\n",
            "0       9.129222  0.725942  0.485812  0.001883  0.000189 -1.130839e-07   \n",
            "1       8.958515  0.739710  0.503838  0.004574  0.000481 -7.124751e-07   \n",
            "2       9.393519  0.705631  0.456746  0.002625  0.000759  8.875469e-07   \n",
            "3       8.795325  0.729460  0.491963  0.001992  0.001841  3.526866e-06   \n",
            "4       9.227785  0.688780  0.431023  0.000185  0.000038  1.094449e-09   \n",
            "\n",
            "        hu5           hu6  \n",
            "0 -0.000130  4.585086e-09  \n",
            "1 -0.000339 -9.235960e-09  \n",
            "2  0.000200 -6.015546e-07  \n",
            "3  0.001291  1.464620e-08  \n",
            "4 -0.000004 -2.982523e-09  \n",
            "\n",
            "--- File: timeseries_data.csv ---\n",
            "First 5 rows:\n",
            "   worm_index  timestamp well_name      speed  angular_velocity  \\\n",
            "0           1          0    b'n/a'        NaN               NaN   \n",
            "1           1          1    b'n/a'  22.786860          0.151661   \n",
            "2           1          2    b'n/a'  33.959213          0.058174   \n",
            "3           1          3    b'n/a'  26.725447         -0.104145   \n",
            "4           1          4    b'n/a'   1.775919          0.082379   \n",
            "\n",
            "   relative_to_body_speed_midbody  relative_to_body_radial_velocity_head_tip  \\\n",
            "0                             NaN                                        NaN   \n",
            "1                       -4.663084                                  -4.953424   \n",
            "2                       -5.766367                                  -9.173655   \n",
            "3                       -5.275574                                  -8.301990   \n",
            "4                        0.020296                                   0.298638   \n",
            "\n",
            "   relative_to_body_angular_velocity_head_tip  \\\n",
            "0                                         NaN   \n",
            "1                                    0.586394   \n",
            "2                                    0.793725   \n",
            "3                                    0.677850   \n",
            "4                                    0.158210   \n",
            "\n",
            "   relative_to_body_radial_velocity_neck  \\\n",
            "0                                    NaN   \n",
            "1                              -1.725676   \n",
            "2                              -0.748221   \n",
            "3                              -1.852820   \n",
            "4                               0.271362   \n",
            "\n",
            "   relative_to_body_angular_velocity_neck  ...  turn  head_tail_distance  \\\n",
            "0                                     NaN  ...   NaN           56.432686   \n",
            "1                               -0.129293  ...   NaN           58.138640   \n",
            "2                               -0.321030  ...   NaN           56.862057   \n",
            "3                               -0.407688  ...   NaN           54.121700   \n",
            "4                                0.042040  ...   NaN           53.757225   \n",
            "\n",
            "   coord_x_body  coord_y_body  coord_x_tail  coord_y_tail  coord_x_midbody  \\\n",
            "0     1205.5984     1561.6873     1183.5992     1571.1593        1205.2391   \n",
            "1     1205.1595     1561.8689     1182.3748     1571.3212        1204.8547   \n",
            "2     1206.5302     1561.4672     1184.3524     1571.5812        1206.2192   \n",
            "3     1211.4097     1558.0873     1191.5598     1571.0522        1211.7067   \n",
            "4     1213.9414     1556.7046     1194.0927     1570.3728        1214.4783   \n",
            "\n",
            "   coord_y_midbody  coord_x_head  coord_y_head  \n",
            "0        1561.0603     1226.3652     1548.0636  \n",
            "1        1561.3326     1226.1028     1548.0239  \n",
            "2        1560.8207     1227.1520     1547.1919  \n",
            "3        1558.7002     1232.0688     1546.3618  \n",
            "4        1557.6810     1235.1306     1546.7496  \n",
            "\n",
            "[5 rows x 153 columns]\n",
            "\n",
            "--- File: dorsal_contours.csv ---\n",
            "First 5 rows:\n",
            "                                     dorsal_contours\n",
            "0  [[1229.0091552734375, 1545.02197265625], [1227...\n",
            "1  [[1230.000732421875, 1545.9783935546875], [122...\n",
            "2  [[1230.0032958984375, 1544.008056640625], [122...\n",
            "3  [[1235.9876708984375, 1547.016357421875], [123...\n",
            "4  [[1239.03173828125, 1547.9832763671875], [1237...\n",
            "\n",
            "--- File: skeletons.csv ---\n",
            "First 5 rows:\n",
            "                                           skeletons\n",
            "0  [[1229.0045166015625, 1545.0218505859375], [12...\n",
            "1  [[1229.9971923828125, 1545.9932861328125], [12...\n",
            "2  [[1230.00732421875, 1543.99755859375], [1229.6...\n",
            "3  [[1236.0045166015625, 1546.9891357421875], [12...\n",
            "4  [[1239.021484375, 1547.977783203125], [1237.77...\n",
            "\n",
            "--- File: ventral_contours.csv ---\n",
            "First 5 rows:\n",
            "                                    ventral_contours\n",
            "0  [[1229.0228271484375, 1544.9942626953125], [12...\n",
            "1  [[1230.0194091796875, 1546.0147705078125], [12...\n",
            "2  [[1230.000732421875, 1543.988525390625], [1230...\n",
            "3  [[1236.0035400390625, 1547.0086669921875], [12...\n",
            "4  [[1239.00341796875, 1548.0078125], [1238.11962...\n",
            "\n",
            "--- File: provenance_tracking.csv ---\n",
            "First 5 rows:\n",
            "      CLASS                     TITLE VERSION\n",
            "0  b'GROUP'  Empty(dtype=dtype('S1'))  b'1.0'\n",
            "\n",
            "Dataset visualization process completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Datasets extracted from the groups"
      ],
      "metadata": {
        "id": "NaLhtaDgieW3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "csv_output_directory = '/content/drive/MyDrive/Worms/Resultados/L2/L2_featuresN/Groups'\n",
        "\n",
        "print(f\"\\n{'='*30}\")\n",
        "print(f\"Visualizing the first 5 rows of the datasets in: {csv_output_directory}\")\n",
        "print(f\"{'='*30}\")\n",
        "try:\n",
        "    # Check if the output directory exists\n",
        "    if not os.path.exists(csv_output_directory):\n",
        "        print(f\"Error: The directory '{csv_output_directory}' was not found.\")\n",
        "        exit()\n",
        "\n",
        "    # Iterate through all files in the specified directory\n",
        "    for filename in os.listdir(csv_output_directory):\n",
        "        if filename.endswith(\".csv\"):  # Check if the file is a CSV file\n",
        "            file_path = os.path.join(csv_output_directory, filename)\n",
        "            try:\n",
        "                # Read the CSV file into a Pandas DataFrame\n",
        "                df = pd.read_csv(file_path)\n",
        "\n",
        "                print(f\"\\n--- File: {filename} ---\")\n",
        "                print(\"First 5 rows:\")\n",
        "                if not df.empty:\n",
        "                    print(df.head())\n",
        "                else:\n",
        "                    print(\"The DataFrame is empty.\")\n",
        "\n",
        "            except pd.errors.EmptyDataError:\n",
        "                print(f\"Warning: The file '{filename}' is empty.\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error reading the file '{filename}': {e}\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "\n",
        "print(\"\\nDataset visualization process completed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VvKQQvhPigpg",
        "outputId": "71fd9ba4-a54a-4cbd-a5f6-6af9e082d333"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==============================\n",
            "Visualizing the first 5 rows of the datasets in: /content/drive/MyDrive/Worms/Resultados/L2/L2_featuresN/Groups\n",
            "==============================\n",
            "\n",
            "--- File: provenance_tracking.csv ---\n",
            "First 5 rows:\n",
            "      CLASS                     TITLE VERSION\n",
            "0  b'GROUP'  Empty(dtype=dtype('S1'))  b'1.0'\n",
            "\n",
            "Dataset visualization process completed.\n"
          ]
        }
      ]
    }
  ]
}